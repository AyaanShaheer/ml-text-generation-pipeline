<img width="2319" height="755" alt="diagram-export-9-24-2025-5_44_59-PM" src="https://github.com/user-attachments/assets/e12df3e0-eb0d-41d0-93bf-43569cd202f7" />



```markdown
# ğŸ¤– AI Text Generator - LSTM Neural Network

> **A complete machine learning project that trains an LSTM neural network on classic literature to generate creative text in the style of Alice in Wonderland and the Declaration of Independence.**





## ğŸŒŸ Project Overview

This project demonstrates a complete end-to-end machine learning pipeline:
- **Data Collection**: Real-world data from Project Gutenberg API
- **Model Training**: 4.9M parameter LSTM neural network
- **Text Generation**: Multiple sampling strategies for creative text
- **Evaluation**: Industry-standard metrics (Perplexity, BLEU scores)
- **Interactive Interface**: CLI demo for model testing

## ğŸ“ˆ Model Performance

| Metric | Score | Status |
|--------|-------|--------|
| **Perplexity** | 161.99 | âœ… Good |
| **BLEU-1** | 0.2608 | âœ… Strong |
| **BLEU-2** | 0.1917 | âœ… Good |
| **Text Diversity** | 0.677 | âœ… Creative |
| **Model Parameters** | 4,916,298 | âœ… Efficient |
| **Vocabulary Size** | 1,610 | âœ… Optimal |

## ğŸš€ Quick Start

### 1. Setup Environment
```
# Clone repository
git clone [<repository-url>](https://github.com/AyaanShaheer/ml-text-generation-pipeline.git)
cd text_generation_ml_project

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Run Data Pipeline
```
python test_pipeline.py
```

### 3. Train Model
```
python train_model.py
```

### 4. Evaluate Model
```
python evaluation.py
```

### 5. Interactive Demo
```
python interactive_demo.py
```

## ğŸ“‚ Project Structure

```
text_generation_ml_project/
â”œâ”€â”€ src/                          # Core ML modules
â”‚   â”œâ”€â”€ data_collector.py        # Project Gutenberg data collection
â”‚   â”œâ”€â”€ text_preprocessor.py     # Text preprocessing & tokenization
â”‚   â”œâ”€â”€ data_pipeline.py         # Complete data pipeline
â”‚   â”œâ”€â”€ model.py                 # LSTM neural network architectures
â”‚   â”œâ”€â”€ trainer.py               # PyTorch training pipeline
â”‚   â””â”€â”€ generator.py             # Text generation with sampling
â”œâ”€â”€ data/                         # Training data storage
â”œâ”€â”€ models/                       # Saved model checkpoints
â”œâ”€â”€ notebooks/                    # Jupyter notebooks for exploration
â”œâ”€â”€ tests/                        # Unit tests
â”œâ”€â”€ train_model.py               # Main training script
â”œâ”€â”€ test_pipeline.py             # Data pipeline testing
â”œâ”€â”€ evaluation.py                # Model evaluation with metrics
â”œâ”€â”€ interactive_demo.py          # CLI interface for text generation
â”œâ”€â”€ evaluation_report.json       # Detailed evaluation results
â””â”€â”€ requirements.txt             # Python dependencies
```

## ğŸ”¬ Technical Details

### Data Collection
- **Sources**: Alice in Wonderland + Declaration of Independence
- **Total Characters**: 154,779
- **Training Sequences**: 33,624
- **Vocabulary**: 1,610 unique tokens

### Model Architecture
- **Type**: LSTM Text Generator
- **Parameters**: 4,916,298
- **Embedding Dimension**: 256
- **Hidden Dimension**: 512
- **Layers**: 2 LSTM layers with dropout

### Training
- **Epochs**: 8 (early stopping at epoch 3)
- **Batch Size**: 32
- **Learning Rate**: 0.001 with scheduling
- **Validation Split**: 10%
- **Best Validation Loss**: 4.5550

### Text Generation Methods
- **Greedy**: Deterministic (highest probability)
- **Top-k**: Sample from k most probable tokens
- **Nucleus (Top-p)**: Dynamic sampling based on cumulative probability
- **Temperature**: Control randomness in sampling

## ğŸ“Š Generated Text Examples

**Prompt**: "Alice fell down the rabbit hole and discovered"

- **Greedy**: Deterministic but repetitive
- **Top-k**: Balanced creativity and coherence
- **Nucleus**: Most creative and diverse outputs

## ğŸ† Key Features

âœ… **End-to-End Pipeline**: Complete ML workflow from data to deployment  
âœ… **Real Data**: Project Gutenberg literature collection  
âœ… **Professional Training**: PyTorch with validation and early stopping  
âœ… **Multiple Architectures**: LSTM, Transformer, Hybrid models  
âœ… **Comprehensive Evaluation**: Perplexity, BLEU, diversity metrics  
âœ… **Interactive Interface**: User-friendly text generation demo  
âœ… **Production Ready**: Proper error handling and logging  

## ğŸ› ï¸ Technologies Used

- **Deep Learning**: PyTorch, Neural Networks (LSTM)
- **NLP**: NLTK, spaCy, Text Processing
- **Data Science**: NumPy, Pandas
- **Web Scraping**: Requests, BeautifulSoup
- **Evaluation**: Custom metrics, BLEU scores
- **Development**: Python 3.8+, Virtual Environments

## ğŸ“ˆ Results Summary

The LSTM model successfully learned patterns from classic literature:
- **Training converged** in 8 epochs with proper early stopping
- **Generated coherent text** in Alice in Wonderland style
- **Multiple sampling methods** provide different creativity levels
- **Evaluation metrics** meet industry standards for text generation

## ğŸ¯ Use Cases

- **Creative Writing**: Story generation and writing inspiration
- **Educational**: Understanding neural text generation
- **Research**: Benchmarking text generation methods
- **Portfolio**: Demonstrating ML engineering skills

## ğŸ“ Future Enhancements

- [ ] Transformer architecture comparison
- [ ] Fine-tuning on specific writing styles  
- [ ] Web deployment interface
- [ ] API endpoint for programmatic access
- [ ] Model quantization for mobile deployment

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

## ğŸ™ Acknowledgments

- **Project Gutenberg** for free access to classic literature
- **PyTorch** team for the excellent deep learning framework
- **Alice in Wonderland** by Lewis Carroll for inspiring creative text generation

---

**Built with â¤ï¸ for ML Engineering Portfolio**
```
